{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-17T00:33:05.730546Z",
     "start_time": "2025-11-17T00:32:57.923941Z"
    }
   },
   "source": [
    "import arxiv\n",
    "\n",
    "query = \"attention transformers\"   # your search query\n",
    "results = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=10,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    ")\n",
    "\n",
    "for paper in results.results():\n",
    "    print(\"Downloading:\", paper.title)\n",
    "    path = paper.download_pdf(dirpath=\"./\")  # saves pdf in ./downloads\n",
    "    print(path)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsouz\\AppData\\Local\\Temp\\ipykernel_33088\\556423516.py:10: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for paper in results.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: Vision Transformer with Quadrangle Attention\n",
      "./2303.15105v1.Vision_Transformer_with_Quadrangle_Attention.pdf\n",
      "Downloading: Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains\n",
      "./2402.04161v2.Attention_with_Markov__A_Framework_for_Principled_Analysis_of_Transformers_via_Markov_Chains.pdf\n",
      "Downloading: Local Attention Transformers for High-Detail Optical Flow Upsampling\n",
      "./2412.06439v1.Local_Attention_Transformers_for_High_Detail_Optical_Flow_Upsampling.pdf\n",
      "Downloading: Forgetting Transformer: Softmax Attention with a Forget Gate\n",
      "./2503.02130v2.Forgetting_Transformer__Softmax_Attention_with_a_Forget_Gate.pdf\n",
      "Downloading: Dilated Neighborhood Attention Transformer\n",
      "./2209.15001v3.Dilated_Neighborhood_Attention_Transformer.pdf\n",
      "Downloading: On the Surprising Effectiveness of Attention Transfer for Vision Transformers\n",
      "./2411.09702v1.On_the_Surprising_Effectiveness_of_Attention_Transfer_for_Vision_Transformers.pdf\n",
      "Downloading: Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition\n",
      "./2204.00452v2.Vision_Transformer_with_Cross_attention_by_Temporal_Shift_for_Efficient_Action_Recognition.pdf\n",
      "Downloading: Efficient Content-Based Sparse Attention with Routing Transformers\n",
      "./2003.05997v5.Efficient_Content_Based_Sparse_Attention_with_Routing_Transformers.pdf\n",
      "Downloading: Visual Attention Network\n",
      "./2202.09741v5.Visual_Attention_Network.pdf\n",
      "Downloading: Single Headed Attention RNN: Stop Thinking With Your Head\n",
      "./1911.11423v2.Single_Headed_Attention_RNN__Stop_Thinking_With_Your_Head.pdf\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T23:58:16.990048Z",
     "start_time": "2025-11-16T23:58:16.985346Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e54aeb4886145c92",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
